# Nano_GPT-GPT from Scratch
# Simplified GPT Implementation in Python

This project implements a **simplified GPT (Generative Pre-trained Transformer)** model from scratch in Python. It serves as an educational walkthrough of the key components involved in building a transformer-based language model. Below is an overview of the project's structure and features:

## Key Components

- **Character Vocabulary**: 
  - Maps characters to integers and vice versa for text processing.
  
- **Encoding/Decoding**:
  - Converts text into numerical data for training and decodes it back into text for generation.

- **Transformer Architecture**:
  - Implements core components such as:
    - **Self-Attention**: Captures contextual relationships between tokens.
    - **Positional Encoding**: Adds positional information to input embeddings.
    - **Multi-Layer Blocks**: Stacks multiple transformer layers for deeper learning.

- **Training Pipeline**:
  - Processes input data, optimizes the model using gradient-based methods, and generates predictions.


